{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e073a32-82ad-4125-85e1-4446a99a7038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 12:27:32 WARN Utils: Your hostname, Arbnors-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.11.6.58 instead (on interface en0)\n",
      "24/06/03 12:27:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/03 12:27:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1f5c54-9860-4942-8a34-8600af713db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 12:27:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Working with Datatypes and Joins\") \\\n",
    "    .config(\"spark.driver.host\", \"driver-hostname\") \\\n",
    "    .config(\"spark.driver.port\", \"driver-port\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a332ba1e-1e9a-4075-b470-fccac6d04d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ff45a4-5dbe-4f73-b2fb-f110aae0426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da981452-be6b-41a6-8ce8-039805de230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .save(\"data/flight-data/csv/testCSV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a5a2c6-5487-4e32-8649-237f27ccdad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.format(\"parquet\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"data/flight-data/parquet/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231274d2-45ff-48cf-8fa6-044fe42367b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad5d5a5-3db5-4d4c-9664-a1e3c9d93578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|doubled_count|count|\n",
      "+-------------+-----+\n",
      "|          262|  131|\n",
      "|            2|    1|\n",
      "|            2|    1|\n",
      "|            2|    1|\n",
      "|            2|    1|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Group by ORIGIN_COUNTRY_NAME and count the occurrences, then select the original count and doubled count\n",
    "result = parquetFile.groupBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .selectExpr(\"count * 2 as doubled_count\", \"count\").sort(col(\"count\"), ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65cb769-1cb3-4eb7-93cb-a7e0bbfe7f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csvFile.repartition(5).write.format(\"csv\").save(\"data/flight-data/csv/partitionData/multiple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbbdd970-6480-4934-8bfd-330ee420fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csvFile.limit(10).write.format(\"parquet\").partitionBy(\"ORIGIN_COUNTRY_NAME\").mode(\"overwrite\").save(\"data/flight-data/csv/partitionData/CountryPartition.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bc170a8-9e02-4f66-b88d-a7731a65ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryData = spark.read.format(\"parquet\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"data/flight-data/csv/partitionData/CountryPartition.parquet/ORIGIN_COUNTRY_NAME=Grenada/part-00000-3b961257-3a57-42c5-975b-34dbf7f66b76.c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04a35bad-5ddf-44e9-bd34-5be42d4ad001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|count|\n",
      "+-----------------+-----+\n",
      "|    United States|   54|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43514fb2-1db3-4e09-915c-11c690f9dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# To find the right number of buckets, user should divide total size of data / 128 MB. 128MB is the size of blocks in HDFS. \n",
    "\n",
    "numberOfBuckets = 5\n",
    "columnToBucket = \"count\"\n",
    "\n",
    "csvFile.write.format(\"parquet\").bucketBy(numberOfBuckets, columnToBucket).saveAsTable(\"bucketedTable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
