{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2723637-1391-4d55-a06e-0d6b86bcc1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 12:50:08 WARN Utils: Your hostname, Arbnors-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.11.6.58 instead (on interface en0)\n",
      "24/06/07 12:50:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/07 12:50:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b430bc6a-0671-4c73-942d-b5d702fa6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0),\n",
       " Row(id=1),\n",
       " Row(id=2),\n",
       " Row(id=3),\n",
       " Row(id=4),\n",
       " Row(id=5),\n",
       " Row(id=6),\n",
       " Row(id=7),\n",
       " Row(id=8),\n",
       " Row(id=9)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5905d6a-1a26-4cfe-bc1b-1bfb88742f49",
   "metadata": {},
   "source": [
    "## One of the easiest ways to get RDDs is from an existing DataFrame or Dataset. Converting these to an RDD is simple: just use the rdd method on any of these data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ac5788-ecee-48fe-b83e-2d87a91388eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).toDF(\"id\").rdd.map(lambda x : x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4345ee1-cadf-486b-a587-be8278ccd764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abe18c-8756-4a91-b962-283663efb7a5",
   "metadata": {},
   "source": [
    "To create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession).\n",
    "\n",
    "This turns a single node collection into a parallel collection.\n",
    "\n",
    "When creating this parallel collection, you can also explicitly state the number of partitions into which you would like to distribute this array. In this case, we are creating two partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c84a4ee-b5fe-4174-b68a-811bf64d320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myCollection = \"My name is Bond, James Bond\"\\\n",
    "    .split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa95c4be-1812-4b48-b6a8-9ffb355742cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd5c253d-8350-40e7-b145-0dca9ab4455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Bond,', 'James', 'Bond']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8199e7-d528-401c-98d3-a9757ebbb9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
