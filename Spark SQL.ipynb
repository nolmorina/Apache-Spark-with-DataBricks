{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794878ce-c04f-4c1b-8cf5-40222464c33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 14:50:59 WARN Utils: Your hostname, Arbnors-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.11.6.58 instead (on interface en0)\n",
      "24/06/03 14:50:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/03 14:51:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/03 14:51:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Initialize Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc1d5c6-bfb9-4417-bdc6-fb0d2836ed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(1 + 1)|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 1+1\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03212ca9-c7f6-441f-bd43-0f13aa80bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"data/flight-data/json/2010-summary.json\").createOrReplaceTempView(\"sqlData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd11ec15-4388-47cf-bb41-9d11688d0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+\n",
      "|DEST_COUNTRY_NAME    |sum(count)|\n",
      "+---------------------+----------+\n",
      "|Sweden               |65        |\n",
      "|Spain                |422       |\n",
      "|Saint Kitts and Nevis|113       |\n",
      "|South Korea          |683       |\n",
      "|Sint Maarten         |61        |\n",
      "+---------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) \n",
    "FROM sqlData \n",
    "GROUP BY DEST_COUNTRY_NAME \n",
    "HAVING sum(count) > 30\"\"\")\\\n",
    ".where(\"DEST_COUNTRY_NAME LIKE 'S%'\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7ce444-5c5b-4fec-a854-53485bd06f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|DEST_COUNTRY_NAME|sum(count)|\n",
      "+-----------------+----------+\n",
      "|Fiji             |53        |\n",
      "|Peru             |212       |\n",
      "|Cuba             |243       |\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) \n",
    "FROM sqlData \n",
    "GROUP BY DEST_COUNTRY_NAME \n",
    "HAVING len(DEST_COUNTRY_NAME) < 5\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a641ea-c9e2-41ae-96b0-be78024ca44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 14:55:56 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider JSON. Persisting data source table `spark_catalog`.`default`.`flights` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new table from the existing one\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE flights (\n",
    "DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "USING JSON OPTIONS (path '/Users/arbnormorina/Desktop/Spark/Apache-Spark-with-DataBricks/data/flight-data/json/2010-summary.json')\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23511813-a89a-49ff-8336-f8f9d987de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM flights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa90f0f-3f10-4eb4-a666-2970ca09a08b",
   "metadata": {},
   "source": [
    "## External Tables or unmanaged table\n",
    "\n",
    "Spark will manage the tableâ€™s metadata; however, the files are not managed by Spark at all. \n",
    "\n",
    "The data will be read but it will not be stored the data are not owned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92905fdc-bc48-48cf-8005-04dd459ed3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE EXTERNAL TABLE flightsExternal(\n",
    "DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "LOCATION '/Users/arbnormorina/Desktop/Spark/Apache-Spark-with-DataBricks/data/flight-data-hive'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04536872-a21f-40e4-afed-03e558780a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "|United States    |Singapore          |1    |\n",
      "|United States    |Grenada            |62   |\n",
      "|Costa Rica       |United States      |588  |\n",
      "|Senegal          |United States      |40   |\n",
      "|Moldova          |United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM flightsExternal\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0e5e254-e731-4bfb-942f-02afcd7f5676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|  DEST_COUNTRY_NAME|   string|   NULL|\n",
      "|ORIGIN_COUNTRY_NAME|   string|   NULL|\n",
      "|              count|   bigint|   NULL|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE flightsExternal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c7b96-4d79-4f50-886b-f1c111e9980d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
